{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef Lenet5_OnMnistDataSet():\n",
    "    import numpy as np\n",
    "    #import tensorflow as tf\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    import numpy as np\n",
    "    tf.disable_v2_behavior() \n",
    "\n",
    "    def load_data(num_classes=10):\n",
    "        (xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.mnist.load_data()\n",
    "        xtrain = xtrain.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "        xtest = xtest.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "        ytrain = np.eye(num_classes)[ytrain] # one hot encoding\n",
    "        ytest = np.eye(num_classes)[ytest]   # one hot encoding\n",
    "        return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "    def next_batch(batch_size, data, labels):\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:batch_size]\n",
    "        data_shuffle = [data[i] for i in idx]\n",
    "        labels_shuffle = [labels[i] for i in idx]\n",
    "        return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load_data()\n",
    "\n",
    "    # Parameters\n",
    "    num_epoch = 4000\n",
    "    batch_size = 128\n",
    "\n",
    "    # layer 0: input data\n",
    "    x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "    y = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "    # layer 1: convolution\n",
    "    # filter size = 5x5, input channel = 1, output channel = 32\n",
    "    conv1_w = tf.get_variable(\"conv1_w\", [5,5,1,32], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv1_b = tf.get_variable(\"conv1_b\", [32], initializer=tf.constant_initializer(value=0))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_w, strides=[1,1,1,1], padding='SAME')\n",
    "    relu1 = tf.nn.relu( tf.nn.bias_add(conv1, conv1_b) )\n",
    "\n",
    "    # layer 2: max pool\n",
    "    # filter size = 2x2, stride = 2\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # layer 3: convolution\n",
    "    # filter size = 5x5, input channel = 32, output channel = 64\n",
    "    conv2_w = tf.get_variable(\"conv2_w\", [5,5,32,64], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv2_b = tf.get_variable(\"conv2_b\", [64], initializer=tf.constant_initializer(value=0))\n",
    "    conv2 = tf.nn.conv2d(pool1, conv2_w, strides=[1,1,1,1], padding='SAME')\n",
    "    relu2 = tf.nn.relu( tf.nn.bias_add(conv2, conv2_b) )\n",
    "\n",
    "    # layer 4: max pool\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # layer 5: fully connected\n",
    "    fc1_w = tf.get_variable(\"fc1_w\", [7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fc1_b = tf.get_variable(\"fc1_b\", [1024], initializer=tf.constant_initializer(value=0.1))\n",
    "    pool2_vector = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    fc1 = tf.nn.relu( tf.matmul(pool2_vector, fc1_w) + fc1_b )\n",
    "\n",
    "    # dropout layer\n",
    "    fc1_dropout = tf.nn.dropout(fc1, 1.0)\n",
    "\n",
    "    # layer 6: fully connected\n",
    "    fc2_w = tf.get_variable(\"fc2_w\", [1024, 10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fc2_b = tf.get_variable(\"fc2_b\", [10], initializer=tf.constant_initializer(value=0.1))\n",
    "    y_hat = tf.matmul(fc1_dropout, fc2_w) + fc2_b\n",
    "\n",
    "    # layer 7: softmax, output layer\n",
    "    pred = tf.nn.softmax(y_hat)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_hat, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # evaluate model\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epoch):\n",
    "            xbatch, ybatch = next_batch(batch_size, xtrain, ytrain)\n",
    "            sess.run(train_op, feed_dict={x: xbatch, y: ybatch})\n",
    "\n",
    "            if ((epoch + 1) % 100 == 0):\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x: xtest, y: ytest})\n",
    "                print(\"epoch \" + str(epoch+1) + \", loss= \" + \"{:.4f}\".format(loss) + \", acc= \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "        # Calculate accuracy for MNIST test images\n",
    "        acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest})\n",
    "        print('test acc=' + '{:.3f}'.format(acc))\n",
    "Lenet5_OnMnistDataSet()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
