{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Miniconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From D:\\Miniconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch 100, loss= 0.1310, acc= 0.959\n",
      "epoch 200, loss= 0.0767, acc= 0.976\n",
      "epoch 300, loss= 0.0568, acc= 0.982\n",
      "epoch 400, loss= 0.0649, acc= 0.979\n",
      "epoch 500, loss= 0.0440, acc= 0.984\n",
      "epoch 600, loss= 0.0430, acc= 0.985\n",
      "epoch 700, loss= 0.0368, acc= 0.987\n",
      "epoch 800, loss= 0.0354, acc= 0.986\n",
      "epoch 900, loss= 0.0382, acc= 0.987\n",
      "epoch 1000, loss= 0.0353, acc= 0.987\n",
      "epoch 1100, loss= 0.0324, acc= 0.988\n",
      "epoch 1200, loss= 0.0348, acc= 0.988\n",
      "epoch 1300, loss= 0.0358, acc= 0.989\n",
      "epoch 1400, loss= 0.0364, acc= 0.988\n",
      "epoch 1500, loss= 0.0332, acc= 0.990\n",
      "epoch 1600, loss= 0.0432, acc= 0.988\n",
      "epoch 1700, loss= 0.0420, acc= 0.987\n",
      "epoch 1800, loss= 0.0338, acc= 0.988\n",
      "epoch 1900, loss= 0.0424, acc= 0.988\n",
      "epoch 2000, loss= 0.0431, acc= 0.987\n",
      "epoch 2100, loss= 0.0349, acc= 0.989\n",
      "epoch 2200, loss= 0.0355, acc= 0.988\n",
      "epoch 2300, loss= 0.0365, acc= 0.988\n",
      "epoch 2400, loss= 0.0395, acc= 0.987\n",
      "epoch 2500, loss= 0.0293, acc= 0.990\n",
      "epoch 2600, loss= 0.0340, acc= 0.990\n",
      "epoch 2700, loss= 0.0373, acc= 0.989\n",
      "epoch 2800, loss= 0.0407, acc= 0.989\n",
      "epoch 2900, loss= 0.0305, acc= 0.991\n",
      "epoch 3000, loss= 0.0344, acc= 0.990\n",
      "epoch 3100, loss= 0.0310, acc= 0.991\n",
      "epoch 3200, loss= 0.0291, acc= 0.991\n",
      "epoch 3300, loss= 0.0259, acc= 0.992\n",
      "epoch 3400, loss= 0.0359, acc= 0.989\n",
      "epoch 3500, loss= 0.0329, acc= 0.990\n",
      "epoch 3600, loss= 0.0306, acc= 0.991\n",
      "epoch 3700, loss= 0.0350, acc= 0.990\n",
      "epoch 3800, loss= 0.0332, acc= 0.991\n",
      "epoch 3900, loss= 0.0536, acc= 0.987\n",
      "epoch 4000, loss= 0.0426, acc= 0.988\n",
      "test acc=0.988\n"
     ]
    }
   ],
   "source": [
    "def Lenet5_OnMnistDataSet():\n",
    "    import numpy as np\n",
    "    #import tensorflow as tf\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    import numpy as np\n",
    "    tf.disable_v2_behavior() \n",
    "\n",
    "    def load_data(num_classes=10):\n",
    "        (xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.mnist.load_data()\n",
    "        xtrain = xtrain.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "        xtest = xtest.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "        ytrain = np.eye(num_classes)[ytrain] # one hot encoding\n",
    "        ytest = np.eye(num_classes)[ytest]   # one hot encoding\n",
    "        return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "    def next_batch(batch_size, data, labels):\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:batch_size]\n",
    "        data_shuffle = [data[i] for i in idx]\n",
    "        labels_shuffle = [labels[i] for i in idx]\n",
    "        return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load_data()\n",
    "\n",
    "    # Parameters\n",
    "    num_epoch = 4000\n",
    "    batch_size = 128\n",
    "\n",
    "    # layer 0: input data\n",
    "    x = tf.placeholder(\"float\", [None,28,28,1])\n",
    "    y = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "    # layer 1: convolution\n",
    "    # filter size = 5x5, input channel = 1, output channel = 32\n",
    "    conv1_w = tf.get_variable(\"conv1_w\", [5,5,1,32], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv1_b = tf.get_variable(\"conv1_b\", [32], initializer=tf.constant_initializer(value=0))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_w, strides=[1,1,1,1], padding='SAME')\n",
    "    relu1 = tf.nn.relu( tf.nn.bias_add(conv1, conv1_b) )\n",
    "\n",
    "    # layer 2: max pool\n",
    "    # filter size = 2x2, stride = 2\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # layer 3: convolution\n",
    "    # filter size = 5x5, input channel = 32, output channel = 64\n",
    "    conv2_w = tf.get_variable(\"conv2_w\", [5,5,32,64], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv2_b = tf.get_variable(\"conv2_b\", [64], initializer=tf.constant_initializer(value=0))\n",
    "    conv2 = tf.nn.conv2d(pool1, conv2_w, strides=[1,1,1,1], padding='SAME')\n",
    "    relu2 = tf.nn.relu( tf.nn.bias_add(conv2, conv2_b) )\n",
    "\n",
    "    # layer 4: max pool\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # layer 5: fully connected\n",
    "    fc1_w = tf.get_variable(\"fc1_w\", [7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fc1_b = tf.get_variable(\"fc1_b\", [1024], initializer=tf.constant_initializer(value=0.1))\n",
    "    pool2_vector = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    fc1 = tf.nn.relu( tf.matmul(pool2_vector, fc1_w) + fc1_b )\n",
    "\n",
    "    # dropout layer\n",
    "    fc1_dropout = tf.nn.dropout(fc1, 1.0)\n",
    "\n",
    "    # layer 6: fully connected\n",
    "    fc2_w = tf.get_variable(\"fc2_w\", [1024, 10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    fc2_b = tf.get_variable(\"fc2_b\", [10], initializer=tf.constant_initializer(value=0.1))\n",
    "    y_hat = tf.matmul(fc1_dropout, fc2_w) + fc2_b\n",
    "\n",
    "    # layer 7: softmax, output layer\n",
    "    pred = tf.nn.softmax(y_hat)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_hat, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # evaluate model\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epoch):\n",
    "            xbatch, ybatch = next_batch(batch_size, xtrain, ytrain)\n",
    "            sess.run(train_op, feed_dict={x: xbatch, y: ybatch})\n",
    "\n",
    "            if ((epoch + 1) % 100 == 0):\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x: xtest, y: ytest})\n",
    "                print(\"epoch \" + str(epoch+1) + \", loss= \" + \"{:.4f}\".format(loss) + \", acc= \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "        # Calculate accuracy for MNIST test images\n",
    "        acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest})\n",
    "        print('test acc=' + '{:.3f}'.format(acc))\n",
    "Lenet5_OnMnistDataSet()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b003d8754f68205f230b91d29752a0899194161977d5c87b83f80e078894fa96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
